---
title: "Calculating Agreement"
author: "Seyram A. Butame"
---

This document details the calculation of the interater reliability of the screeners in the systematic review project. The `kappa-statistic` is a measure of the IRR or level of agreement. 

I am passing this calculation through stata (so that the rest of the team can follow, however data manipulation is in `R`)

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(RStata) ## For running STATA through R
```

## Read In Data

The data comes from the SysRev Platform. The Platform provides a concordance measurement. However, the Cochrane Handbook recommends providing a "kappa-statistic" as a measurement of the agreement. The data below is taken from the platform, wherein we can see whether each of the screeners decided to include/exclude the reference. Before calculating the kappa-statistic, I need to transform the data a bit:

1. To hide the identities of the screeners
2. To present the data in a format that can be used in the calucation (I beleive that is long to wide format)

```{r, data, message=FALSE, warning=FALSE}

t1 <- read_csv("kappa_draftv2.csv") %>%
  as_tibble() %>%
  mutate(rater=replace(rater, rater=="saweber", "rater1")) %>%
  mutate(rater=replace(rater, rater=="lalarios", "rater2")) %>%
  mutate(rater=replace(rater, rater=="devinl", "rater3")) %>%
  mutate(include=replace(include, include=="FALSE", 0)) %>%
  mutate(include=replace(include, include=="TRUE", 1))%>%
  pivot_wider(names_from = rater, values_from = include) %>%
  arrange(article_id, rater1, rater2, rater3)

t1 <- write_csv(t1, "irr_data.csv")

```

```{r, stata, warning=FALSE, message=FALSE}

options("RStata.StataPath" = "/Applications/Stata/StataBE")
options("RStata.StataVersion" = 17)


```




